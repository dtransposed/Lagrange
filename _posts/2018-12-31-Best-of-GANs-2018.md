---
layout: post
title: "The best of GAN papers in the year 2018"
author: "Damian Bogunowicz"
categories: blog
tags: [computer vision, neural networks, generative adversarial networks]
image: gan.jpg
---

## [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/pdf/1812.04948.pdf)


### Main idea:

This work proposes an alternative view on GAN framework. More specifically, it draws inspiration from the style-transfer design to create a generator architecture, which can learn the difference between high-level attributes (such as age, identity when trained on human faces or background, camera viewpoint, style for bed images) and stochastic variation (freckles, hair details for human faces or colours, fabrics when trained on bed images) in the generated images. Not only it learns to separate those attributes automatically, but it also allows us to control the synthesis in a very intuitive manner.

### The method:

In the classical GAN approach, the generator takes some latent code as an input and outputs an image, which belongs to the distribution it has learned during the training phase. The authors depart from this design by creating a style-based generator, comprised of two elements: 
1. A fully connected network, which represents the non-linear mapping $$f:\mathcal{Z} \rightarrow \mathcal{W}$$ 
2. A synthesis network $$g$$. 


__Fully connected network__ - By transforming a normalized latent vector $$\textbf{z} \in \mathcal{Z}$$, we obtain an intermediate latent vector $$\textbf{w} = f(\textbf{z})$$. The intermidiate latent space $$\mathcal{W}$$ effectively controls the style of the generator. As a side note - the authors make sure to avoid areas of low density of $$\mathcal{W}$$. While this causes some loss of variation in samples $$\textbf{w}$$, it is said to ultimately result in better average image quality. 
Now, a latent vector $$\textbf{w}$$ sampled from intermidiate latent space is being fed into the block 'A' (learned affine transform) and translated into a style $$\textbf{y} =(\textbf{y}_s,\textbf{y}_b)$$. The style is finally injected into the synthesis network through adaptive instance normalization (AdaIN) at each convolution layer. The AdaIN operation is defined as:

$$AdaIN(\textbf{x}_i,\textbf{y})=\textbf{y}_{s}\frac{\textbf{x}_i-\mu(\textbf{x}_i)}{(\sigma(\textbf{x}_i)}+\textbf{y}_b$$

__Synthesis network__ - AdaIN operation alters each feature map $$\textbf{x}_i$$ by normalizing it, and then scaling and shifting using the components from the style $$\textbf{y}$$. Finally, the feature maps of the generator are also being fed a direct means to generate stochastic details - explicit noise input - in the form of single-channel images containing uncorrelated Gaussian noise.

To sum up, while the explicit noise input may be viewed as a 'seed' for the generation process in the synthesis network, the latent code sampled from $$\mathcal{W}$$ injects a certain style to an image using the AdaIN operator. I think that the most interesting part of the paper is, that we can control how this style would actually affect our image.

### Results:


The authors alter the famous Progressive GAN setup (link), but they hold on to the majority of the architecture and hyperparameters.

Style mixing. 
One of the most interesting part of the paper is the ability to inject different styles to the same image at randomly selected point in the synthesis network.During the training, we run two latent codes z1 and z2 through the mapping network and receive corresponding w1 and w2 vectors. We first inject w1 style at some point and then w2. This not only serves as a form of regularizaiton (we convince the networks that styles are uncorrelated), but results in stunning visual effect. 
The image generated purely by z1 is known as the destination. It is a high-resolution, nice looking image. Same a applies to an image created by z2, which is called a source. Now, during the generation of the destination, we may at some point inject the z2 code, to override a subset of styles present in the destination with those of the source. The influence of the source on the destination is controlled by the location of layers which are being “nurtured” with the latent code of the source. This way, we can decide to what extended we want to affect the destination image. We may want to control the high level aspects (such as hair style, glasses or age), smaller scale facial features (hair style details, eyes) or just change small details such as hair colour, tone of skin complexion or skin structure.

Stochastic variation

disentaglement


## [Evolutionary Generative Adversarial Networks](https://arxiv.org/abs/1803.00657)

### Main idea:
In the classical setting GANs are being trained by alternately updating a generator and discriminator using back-propagation. This two-player minmax game is being implemented by utilizing the cross-entropy mechanism in the objective function. 
The authors of E-GAN propose the alternative GAN framework which is based on evolutionary algorithms. They attempt to restate loss function in form of an evolutionary problem. The task of the generator is to undergo constant mutation under the influence of the discriminator. According to the principle of <em>“survival of the fittest”</em>, one hopes that the last generation of generators would <em>“evolve”</em> in such a way, that it learns the correct distribution of training samples.

### The method:
An evolutionary algorithm attempts to evolve a population of generators in a given environment (here, the discriminator). Each individual from the population represents a possible solution in the parameter space of the generative network. The evolution process boils down to three steps:

{:refdef: style="text-align: center;"}
![alt text](https://raw.githubusercontent.com/dtransposed/dtransposed.github.io/master/assets/4/1.jpg){:height="100%" width="100%"}
{: refdef}
<em>The original GAN framework vs E-GAN framework.</em>


1. Variation: A generator individual $$G_{\theta}$$ produces its children $$G_{\theta_{0}}, G_{\theta_{1}}, G_{\theta_{2}}, ...$$ by modifying itself according to some mutation properties.
2. Evaluation: Each child is being evaluated using a fitness function, which depends on the current state of the discriminator
3. Selection: We assess each child and decide if it did good enough on the evaluation. If yes, it is being kept, otherwise we discard it.

I have mentioned two concepts which should be discussed in more detail: mutation properties and a fitness function.

__Mutations__ - those are the changes introduced to the children in the variation step. There are inspired by original GAN training objectives. The authors have distinguished three, the most effective, types of mutations. Those were minmax mutation (which encourages minimization of Jensen-Shannon divergence), heuristic mutation (which adds inverted Kullback-Leibler divergence term) and least-squares mutation (inspired by [LSGAN](https://arxiv.org/abs/1611.04076)).
__Fitness__ function - in evolutionary algorithm tell us how close a given  child is to achieving the set aim. Here, it is a sum of quality fitness score and diversity fitness score. The former makes sure, that generator comes up with outputs which can fool the discriminator, while the latterpays attention to the diversity of generated samples. 
So one hand, the offsprings are being taught not only to approximate the original distribution well, but also to remain diverse and avoid the mode collapse trap.

The authors claim that their approach tackles multiple, well-known problems. E-GANs not only do better in terms of stability and suppressing mode collapse, it also alleviates the burden of careful choice of hyperparameters and architecture (critical for the convergence). 
Finally, the authors claim the E-GAN converges faster that the conventional GAN framework.

### Results:
The algorithm has been tested not only on synthetic data, but also against CIFAR-10 dataset and Inception score. The authors have modified the popular GAN methods such as [DCGAN](https://arxiv.org/abs/1511.06434) and tested them on real-life datasets. The results indicate, that  E-GAN can be trained to generate diverse, high-quality images from the target data distribution. By scrutinizing the space continuity, we can discover, that their solution has indeed learned a meaningful projection from latent noisy space to image space. By interpolating between latent vectors we can obtain generated images which smoothly change semantically meaningful face attributes.

{:refdef: style="text-align: center;"}
![alt text](https://raw.githubusercontent.com/dtransposed/dtransposed.github.io/master/assets/4/2.jpg){:height="100%" width="100%"}
{: refdef}
<em>Linear interpolating in latent space. The generator has learned distribution of images from CelebA dataset.</em>
And the last observation - it is really interesting, that it is enough to preserve only one child in every selection step to successfully traverse the parameter space towards the optimal solution.
