---
layout: post
title: "The best of GAN papers in the year 2018"
author: "Damian Bogunowicz"
categories: blog
tags: [computer vision, neural networks, generative adversarial networks]
image: gan.jpg
---


##Evolutionary generative Adversarial Networks (https://arxiv.org/abs/1803.00657)

#Main idea:
In the classical setting, GANs are being trained by alternately updating a generator and discriminator using back-propagation. This two-player minmax game is being implemented by utilizing the cross-entropy mechanism in the objective function. 
The authors of E-GAN propose the alternative method of GAN framework, which is based on evolutionary algorithms. They attempt to restate loss function in form of a evolutionary problem. The task of the generator is to undergo constant mutation under the influence of the discriminator. According to the principle of “survival of the fittest”, one hopes that the last generation of generators would “evolve” in such way, that it learns the distribution of training samples.

#The method:
An evolutionary algorithm attempts to evolve a population of generators in a given environment (here, the discriminator). Each individual from the population represents a possible solution in the parameter space of the generative network. The evolution process is comprised out of three steps (see figure x).

Variation: A generator individual G_theta produces its children by modifying itself according to mutation properties.
Evaluation: Each child is being evaluated using a fitness function, which depends on the current state of the discriminator
Selection: Here, we assess each child and decide if it did good enough on the evaluation. If yes, it is being kept, otherwise discarded.

I have mentioned two notions which should be discussed in more detail: mutation properties and a fitness function. Let’s take a look at mutations first. Those are the changes introduced to the children, which are being inspired by original GAN training objectives.The authors have distinguished three types of mutations, which were the most effective. Those were minmax mutation (which encourages minimization of Jensen-Shannon divergence), heuristic mutation (which adds inverted Kullback-Leibler divergence term to the previous method)  and least-squares mutation (inspired by LSGAN (reference)).
When it comes to the fitness function, it is a sum of quality fitness score and diversity fitness score. The prior element makes sure, that generator comes up with outputs which can fool the discriminator, while the latter term pays attention to the diversity of generated samples. So one hand, the offsprings are being taught not only to approximate the original distribution well, but also to remain stable and avoid the mode collapse trap.

The authors claim that their approach tackles multiple problems characteristic for GANs. E-GANs not only do better in terms of instability and suppressing mode collapse, it also alleviates the burden of careful choice of hyperparameters and architecture (critical for the convergence. Finally, it is also proven, that this approach converges faster that the conventional GAN framework.

#Results:
The algorithm has been tested not only on synthetic data, but also against CIFAR-10 dataset and Inception score. The authors have also modified the popular GAN methods such as DCGAN or LSGAN using their evolutionary approach and tested them on real-life datasets. This way it has been proven, that E-GAN can be trained to generate diversity and high-quality images from the target data distribution. It is really interesting to see, that their solution has indeed learned a meaningful projection from latent noisy space to image space by scrutinizing the space continuity. By interpolating between latent vectors, we can obtain generated samples which smoothly change semantically meaningful face attributes.
And the last observation - it is really interesting, that it is enough to preserve only one child in every selection step to successfully traverse the parameter space towards the optimal solution.
