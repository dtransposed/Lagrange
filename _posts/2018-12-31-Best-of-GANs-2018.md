---
layout: post
title: "The best of GAN papers in the year 2018"
author: "Damian Bogunowicz"
categories: blog
tags: [computer vision, neural networks, generative adversarial networks]
image: gan.jpg
---

## [Evolutionary Generative Adversarial Networks](https://arxiv.org/abs/1803.00657)

# Main idea:
In the classical setting GANs are being trained by alternately updating a generator and discriminator using back-propagation. This two-player minmax game is being implemented by utilizing the cross-entropy mechanism in the objective function. 
The authors of E-GAN propose the alternative GAN framework which is based on evolutionary algorithms. They attempt to restate loss function in form of an evolutionary problem. The task of the generator is to undergo constant mutation under the influence of the discriminator. According to the principle of <em>“survival of the fittest”</em>, one hopes that the last generation of generators would <em>“evolve”</em> in such a way, that it learns the correct distribution of training samples.

# The method:
An evolutionary algorithm attempts to evolve a population of generators in a given environment (here, the discriminator). Each individual from the population represents a possible solution in the parameter space of the generative network. The evolution process boils down to three steps:

{:refdef: style="text-align: center;"}
![alt text](https://raw.githubusercontent.com/dtransposed/dtransposed.github.io/master/assets/4/1.jpg){:height="100%" width="100%"}
{: refdef}
<em>The original GAN framework vs E-GAN framework.</em>


1. Variation: A generator individual $$G_{\theta}$$ produces its children $$G_{\theta_{0}}, G_{\theta_{1}}, G_{\theta_{2}}, ...$$ by modifying itself according to some mutation properties.
2. Evaluation: Each child is being evaluated using a fitness function, which depends on the current state of the discriminator
3. Selection: We assess each child and decide if it did good enough on the evaluation. If yes, it is being kept, otherwise we discard it.

I have mentioned two concepts which should be discussed in more detail: mutation properties and a fitness function.

Mutations - those are the changes introduced to the children in the variation step. There are inspired by original GAN training objectives. The authors have distinguished three, the most effective, types of mutations. Those were minmax mutation (which encourages minimization of Jensen-Shannon divergence), heuristic mutation (which adds inverted Kullback-Leibler divergence term) and least-squares mutation (inspired by [LSGAN](https://arxiv.org/abs/1611.04076)).
When it comes to the fitness function, it is a sum of quality fitness score and diversity fitness score. The former makes sure, that generator comes up with outputs which can fool the discriminator, while the latterpays attention to the diversity of generated samples. 
So one hand, the offsprings are being taught not only to approximate the original distribution well, but also to remain diverse and avoid the mode collapse trap.

The authors claim that their approach tackles multiple, well-known problems. E-GANs not only do better in terms of stability and suppressing mode collapse, it also alleviates the burden of careful choice of hyperparameters and architecture (critical for the convergence). 
Finally, the authors claim the E-GAN converges faster that the conventional GAN framework.

# Results:
The algorithm has been tested not only on synthetic data, but also against CIFAR-10 dataset and Inception score. The authors have modified the popular GAN methods such as [DCGAN](https://arxiv.org/abs/1511.06434) and tested them on real-life datasets. The results indicate, that  E-GAN can be trained to generate diverse, high-quality images from the target data distribution. By scrutinizing the space continuity, we can discover, that their solution has indeed learned a meaningful projection from latent noisy space to image space. By interpolating between latent vectors we can obtain generated images which smoothly change semantically meaningful face attributes.

{:refdef: style="text-align: center;"}
![alt text](https://raw.githubusercontent.com/dtransposed/dtransposed.github.io/master/assets/4/2.jpg){:height="100%" width="100%"}
{: refdef}
<em>Linear interpolating in latent space. The generator has learned distribution of images from CelebA dataset.</em>
And the last observation - it is really interesting, that it is enough to preserve only one child in every selection step to successfully traverse the parameter space towards the optimal solution.
