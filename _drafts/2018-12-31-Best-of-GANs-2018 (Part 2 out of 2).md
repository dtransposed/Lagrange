---
layout: post
title: "Project - Reinforcement Learning with Unity 3D: G.E.A.R"
author: "Damian Bogunowicz"
categories: blog
tags: [computer vision, neural networks, robotics, reinforcement learning, unity]
image: gear-cover.png
---

Over the course of the last several months I was working on a fantastic project organized by the [Chair for Computer Aided Medical Procedures & Augmented Reality](http://campar.in.tum.de/WebHome). As a result, together with a team of students, we have developed a prototype of an autonomous, intelligent agent for garbage collection. The idea has been born during a day workshop organized by the PhD students from the Technical University of Munich. This was a start of a great journey, which required us to use our knowledge from the fields of Computer Vision, Deep Reinforcement Learning and Game Developement to create a functional simulation of our robot, G.E.A.R - Garbage Evaporating Autonomous Robot. 
This blog post presents the details of our project. Naturally, if you would like to tinker with the G.E.A.R or contribute further to our project, we have set a relevant repository: [Reinforcement Learning With Unity-G.E.A.R](https://github.com/dtransposed/Reinforcement-Learning-With-Unity-G.E.A.R)


## Table of Contents

1. Project overview
	1. Motivation
	2. Agent and Environment
	3. Perception, Cognition, Action
	4. Punishments and Rewards
2. Software and Algorithms
	1. Unity 3D and ML-Agents
	2. Semantic Segmentation
	3. Algorithms for Agent's Brain
3. Presentation of Solutions:
	1. PPO
	2. PPO with Segmentation Network
	3. Behavioral Cloning
	4. Heuristic
	5. Alternative possible solutions
	6. The code in Python
3. 
 presentation


## Project overview

### Motivation:

No matter where you come from - the first things which comes to mind when you hear about Munich, the capital of Bavaria, is Oktoberfest. The famous beer festival is deeply rooted in the Bavarian culture. The scale of the event is impressive: the amount of people which visit Munich in autumn every year, the litres of beer drank by the visitors and the money which exchanges hands during the Oktoberfest! Those numbers can be hardly compared to any other event in the world. Well, clearly one could find a financial incentive to be a part of this huge celebration. Additionally, as an engineer and researcher, my task is to solve (meaningful) problems. In this case, I could spot one instantly.

::::IMAGE TRASH::::

Oktoberfest is indeed exciting and fun for the participants. However, we tend to turn blind eye to things, which happen after the celebration is over. One of those things are the massive amounts of garbage generated each day of the Oktoberfest. At 10pm, when the visitors leave the <em> Wiesn </em>(the area where the Oktoberfest takes place), an army of sanitation workers rushes to clean up the garbage generated by a plutoon of drunk guests. So far, this process is pretty much done by human workers. How about automating the task? Frankly speaking, this not only a mundane and unfulfilling job. This is also a task which could be done much more efficient by robots. Especially by a hive of intelligent, autonomous robots, which can work 24/7. Such a collective of small robotic workers could accelerate the process of garbage collection by orders of magnitude, while simultanously being very cost efficient. So let's take a shot!

::::GIF SWARM::::
<em> A swarm of synchronized, automnomous robots is capable of outperforming a human workers by far.</em>

Our first step is to simulate the robot by using Unity 3D game engine.

### Agent and Environment:

The setup for the agent is a Bavarian-themed room with. The goal of a robot is to explore the environment and learn the proper reasoning (policy), which we may enforce on it indirectly through a set of rewards and punishments. 

The goal of the robot is:
- to approach and gather the collectibles (stale loaves of bread, red plastic cups and white sausages).
- not to collide with static objects (chairs and tables), slamming against the wall or collecting wooden trays (they belong to the owner of a Bavarian tent and should not be collected by the robot for future disposal).

The robot itself is modelled as a cube, which can roam around the room and collect relevant objects. It's action vector contains three elements, which are responsible for: 
- translational motion (move forward, backward, or stay in place)
- rotation (turn left, right or refuse to rotate)
- grabbing state (activate or not)

While the first two actions are pretty straightforward, one could ask what what "grabbing state" is. Since the creation of an actual mechanism for garbage collection would not only be very time-consuming but also troublesome (Unity 3D is not as accurate as CAD software when it comes to modelling the physics of rigid bodies), we have decided to use a certain heuristic to simulate the collecting of an garbage. Every time the robot decides to collect an object, two requirements must be fulfilled:
1. The object must be close to the front part of the robot (confined within the green volume)
2. The robot must decide to activate "a grabber". When the grabbing state is activated, the color of the robot changes from white to red. 

This heuristic not only allows us to model the behaviour of an agent without an actual mechanical implementation of a grabber, but also allows to observe the reasoning of the agent and debug the behaviour of G.E.A.R.

### Perception, Cognition, Action

::::IMAGE PCA::::

<em> Graphical visualization of the perception, cognition and action cycle of G.E.A.R</em>

An intelligent system can be abstracted as an interplay of three systems : perception, cognition and intelligence. In case of G.E.A.R, the __perception__ is handled by the Intel RealSense camera. In every timestep, we simulate the input from the sensor by providing the robot with two pieces of information: an RGB frame as well as depth map. Now __cognition__ comes into play. The RBG input is transformed into semantic segmentation maps which assign a class to every object in the image. This way robot knows what is a semantic meaning of each pixel of the RBG frame. Then the depth and semantic segmentation maps are fused together and analyzed by the set of neural networks - the brain of the robot. Finally, the brain outputs an decision about robot's __action__. 

## Software and Algorithms

### Semantic Segmentation

The robot itself does not know which object should be collected and which should be avoided. This information is obtained from Semantic Segmentation network, which maps the RBG image to a semantic segmentation maps. For the purpose of the project, we have created a dataset of 3007 pairs of images -RBG frames (input) and matching RGB images, where pixel colors correspond to the class of the object (ground truth obtained from from Unity 3D custom shader). We have used [Semantic Segmentation Suite](https://github.com/GeorgeSeif/Semantic-Segmentation-Suite) to quickly train the [SegNet](https://arxiv.org/pdf/1511.00561.pdf)(Badrinarayan et al., 2015) model using our data. Even though SegNet is far from state of the art, given it's simple structure (easy to debug and modify), relatively uncomplicated domain of the problem (artificial images, simple lightning conditions, repeatable environment) and additional requirements (as little overhead as possible), turned out to be a good choice.

### Algorithms for Agent's Brain

As mentioned before, the central part of robot's cognition is the brain. This is the part responsible for the agent's decision: given the current state of the world and my policy, which action should I take? To answer this questions, we have decided to employ several approaches:

__Proximal Policy Optimization__ - 

__Behavioral Cloning__


According to the Reinforcement Learning paradigm, the robot should be able to learn the proper policy through interaction with the environment and collection of feedback signals. For our agent, those signals span from -1 to 0 (punishments) and from 1 to 0 (rewards).

The proper assignment of punishment and rewards and defining the values is challenging. During the project we have learned two important lessons. Those may not be applicable for any RL project, but should be kept in mind for engineers who struggle with the similar challange as ours:

__First, Guide the agent towards the goal first and

__Curriculum learning is great when the Idea is to train with the more easier classes or task initially, and when the model has started to learn those tasks, we can gradually insert more and more complexity into the training data. 

In the end, we have finished the training with following set of rewards and punishments enforced on the agent:

Action           | Signal (Punishment or Reward)         | Comment              |
--------------------- | :-------------------: | :-------------------- |
Gathering the collectible                 |$$+++$$ | The main goal is to collect the garbage    | 
Moving foreward | $$+$$ | Typically assigned in locomotion tasks|
Punishment per step | $$-$$ | So that agent has an incentive to finish the task quickly |
Activating the grabing mechanism | $$-$$ | In real world, activating the grabber mechanism would be ridicolously energy inefficient | 
Colliding with an obstacle | $$--$$ | The initial punishment was low, so the robot learns not to avoid the furniture to manouver between table legs etc. | 
Slamming against a wall | $$--$$ | In rare cases robot can touch the wall, e.g. to pick an object beside it | 
Collecting a wooden tray | $$---$$ | The robot needs to learn not to collect non-collectible items | 

Additionally, there are many useful [tips and tricks regarding the training procedure](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Best-Practices.md) suggested by the authors of ML-Agents Toolkit.


In contrast to other papers I evaluated, the significance of this research does not come from any significant modification to the GAN framework. Here, the major contribution comes from using massive amounts of computational power available (courtesy of Google) to make the training more powerful. This involves using larger models (4-fold increase of network parameters with respect to prior art) and larger batches (increase by almost an order of magnitude). This turns out to be very beneficial:
1. Using large batch sizes (2048 images in one batch) allows every batch to cover more modes. This way the discriminator and generator benefit from better gradients.
2. Doubling the width (number of channels) in every layer increases the capacity of the model and thus contributes to much better performance. Interestingly, increasing the depth has negative influence on the performance.
3. Additional use of class embeddings accelerates the training procedure. Class embeddings mean conditioning the output of the generator on dataset's class labels.
4. Finally, the method also benefits from hierarchical latent spaces - injecting the noise vector $$\textbf{z}$$ into multiple layers rather then solely at the initial layer. This not only improves performance of the network, but also accelerates the training process.

### Results:

Large scale training allows for superior quality of generated images. However, it comes with its own challenges, such as instability. The authors show, that even though the stability can be enforced through regularization methods (especially on the discriminator), the quality of the network is bound to suffer. The clever workaround is to relax the constraints on the weights and allow for training to collapse at the later stages. Then, we may apply the early stopping technique to pick the set of weights just before the collapse. Those weights are usually sufficiently good to achieve impressive results.

{:refdef: style="text-align: center;"}
![alt text](/assets/5/1.png)
{: refdef}
<em> One generated image and its nearest neighbours from ImageNet dataset. Which image is artificially generated? The burger in the top left corner...</em> 

{:refdef: style="text-align: center;"}
![alt text](/assets/5/2.png)
{: refdef}
<em> Great interpolation ability in both class and latent space confirms that the model does not simply memorize data. It is capable of coming up with its own, incredible inventions!</em> 

{:refdef: style="text-align: center;"}
![alt text](/assets/5/3.png)
{: refdef}
<em> While it may be tempting to cherry-pick the best results, the authors of the paper also comment on the failure cases. While easy classes such as a) allow for seamless image generation, difficult classes b) are tough for the generator to reproduce. There are many factors which influence this phenomenon e.g. how well the class is represented in the dataset or how sensitive our eyes to particular objects. While small flaws in the landscape image are unlikely to draw our attention, we are very vigilant towards "weird" human faces or poses. </em>

## [The relativistic discriminator: a key element missing from standard GAN](https://arxiv.org/pdf/1807.00734.pdf)

### Details
The paper has been submitted on 02.06.2018. One of the reasons why this research is impressive is the fact, that it seems that the whole job was done by one person. The author thought about everything - writing a short blog post about [her invention] (https://ajolicoeur.wordpress.com/relativisticgan/), publish well documented [source code](https://github.com/AlexiaJM/RelativisticGAN) and spark an interesting [discussion on reddit](https://www.reddit.com/r/MachineLearning/comments/8vr9am/r_the_relativistic_discriminator_a_key_element/).

### Main idea:

In standard generative adversarial networks, the discriminator $$D$$ estimates the probability of the input data being real or not. The generator $$G$$ tries to increase the probability that generated data is real. During training, in every iteration, we input two equal-sized batches of data into the discriminator: one batch comes from a real distribution $$\mathbb{P}$$, the other from fake distribution $$\mathbb{Q}$$. 
This valuable piece of information, that half of the examined data comes from fake distribution is usually not conveyed in the algorithm. Additionally, in standard GAN framework, the generator attempts to make fake images look more real, but there is no notion that the generated images can be actually “more real” then real images. The author claims that those are the missing pieces, which should have been incorporated into standard GAN framework in the first place. Due to those limitations, it is suggested that training the generator should not only increase the probability that fake data is real but also decrease the probability that real data is real. This observation is also motivated by the IPM-based GANs, which actually benefit from the presence of relativistic discriminator.

### The method:

In order to shift from standard GAN into “relativistic” GAN, we need to modify the discriminator. A very simple example of a Relativistic GAN (RGAN) can be conceptualized in a following way:

In __standard formulation__, the discriminator is a function $$D(x) = \sigma(C(x))$$. $$x$$ is an image (real or fake), $$C(x)$$ is a function which assigns a score to the input image (evaluates how realistic $$x$$ is) and $$\sigma$$ translates the score into a probability between zero to one. If discriminator receives an image which looks fake, it would assign a very low score and thus low probability e.g. $$D(x) = \sigma(-10)=0$$. On the contrary, real-looking input gives us high score and high probability e.g. $$D(x) = \sigma(5)=1$$.

Now, in __relativistic GAN__, the discriminator estimates the probability that the given real data $$x_r$$ is more realistic then a randomly sampled fake data $$x_f$$:

$$D(\widetilde{x}) = \sigma(C(x_r)-C(x_f))$$

To make the relativistic discriminator act more globally and avoid randomness when sampling pairs, the author builds up on this concept to create a __Relativistic average Discriminator__ (RaD). 

$$\bar{D}(x)=\begin{cases}
sigma(C(x)-\mathop{\mathbb{E}}_{x_{f}\sim\mathbb{Q}}C(x_{f})), & \text{if $x_f$ is real}\\
sigma(C(x)-\mathop{\mathbb{E}}_{x_{f}\sim\mathbb{P}}C(x_{f})), & \text{if $x_r$ is fake}.
 \end{cases}$$

This means that whenever the discriminator $$D\hat$$ receives a real image, it evaluates how is this image more realistic that the average fake image from the batch in this iteration. Analogously, $$D\hat$$ receives a fake image, it is being compared to an average of all real images in a batch. This formulation of relativistic discriminator allows us to indirectly compare all possible combinations of real and fake data in the minibatch, without enforcing quadratic time complexity on the algorithm. 


### Results:

{:refdef: style="text-align: center;"}
![alt text](/assets/5/4.png)
{: refdef}
<em>The diagram shows an example of the discriminator’s output in standard GAN: $$P(x_r \text{is real|) = \sigma(C(x_r)))$$ 
and RaD: P(x_r \text{is real}|C(x_f)) = \sigma(C(x_r) − C(x_f))). $$x_f$$ are dogs images while $$x_r$$ are pictures of bread.
I think that this example gives a very good intuitive understanding of the relativistic disciminator.</em>
{:refdef: style="text-align: center;"}
![alt text](/assets/5/5.png)
{: refdef}
<em>Artificially created cats (128x128 resolution), the output from RaLSGAN. Not only the standard LSGAN produces less realistic images, it is also much more unstable.</em>
 
I have the impression that this paper may start a new trend - using relativistic discriminator in different GAN problems. The experiments indicate, that the approach may help with many problems such as stability or inferior image quality. It may also accelerate the networks' training speed. I really love the fact, that the author has questioned a very fundamental element of the GAN architecture. It is exciting to see that there are already state-of-the-art publications which use advantage of relativistic discriminators (even though this paper came out in June). An example of such an architecture is...
 
 
## [ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks](https://arxiv.org/pdf/1809.00219.pdf)

### Details
The paper has been submitted on 17.09.2018. The code is available publicly [on github](https://github.com/xinntao/ESRGAN). Fun fact: several people have used ESRGAN to improve textures in some old games e.g [Morrowind](https://www.youtube.com/watch?v=PupePmY9OA8&t=184s), [Doom 2](https://www.youtube.com/watch?v=u9S8lnGqKkg&t=64s) or [Return to Castle Wolfenstein](https://www.youtube.com/watch?v=uyRfptKJutU).

### Main idea:

The SRGAN was 2017's state of the art invention in the domain of super-resolution (SR) algorithms. It's task was to take a low resolution (LR) image and output its high resolution (HR) representation. The first optimization target of the network was to __minimize the mean squared error (MSE)__ between recovered HR image and the ground truth. This is equivalent to maximizing peak signal-to-noise ratio (PSNR), which is a common measure used to evaluate SR algorithms. However, this favours overly smooth textures. That is why the second goal of the network was to __minimize perceptual loss__. This helps in capturing texture details and high frequency content. 
As the result, the network has learned to find a sweet spot between those two contradictory goals. By forcing the GAN to keep track of goals, the network has learned to produce high quality HR representation of the LR input. 
One year later, the SRGAN method (created by the scientists from Twitter), has been improved by Chinese and Singaporean researchers. The new network can create even more realistic textures with reduced number of artifacts. This has been achieved through several clever tricks.

{:refdef: style="text-align: center;"}
![alt text](/assets/5/6.png)
{: refdef}
<em>Output from SRGAN versus output from ESRGAN, with ground truth as reference. The generated HR image is four times larger than the LR input. The ESRGAN outperforms its predecessor in sharpness and details.</em>

{:refdef: style="text-align: center;"}
![alt text](/assets/5/7.png)
{: refdef}
<em>SRGAN is based on the ResNet architecture. Even though ESRGAN has similar design, it introduces some changes to Basic Blocks -shifts  from Residual Blocks to Residual in Residual Dense Blocks (RRDB)-for better performance.</em>

### The method:

The ESRGAN takes SRGAN and employs several clever tricks to improve the quality of the generated images. Those four improvements are:
1. Introducing changes to the generator's architecture (switching from Residual Blocks to RRDB, removing batch normalization).
2. Replacing an ordinary discriminator with the relativistic discriminator (as described in the previously discussed paper).
3. Regarding perceptual loss, using feature maps before activation, rather then post-activation.
4. Pre-training the network to first optimize for PSNR and then fine tune it with the GAN.

{:refdef: style="text-align: center;"}
![alt text](/assets/5/8.png)
{: refdef}
<em>First, we remove batch normalization from the network. Secondly, we introduce RRDB which combines multi-level residual network and dense connections. This gives the network higher capacity to capture information.</em> 

__Introducing major changes to the network architecture__ - while the generator in the original SRGAN was using residual blocks, the ESRGAN additionally benefits from dense connections (as proposed by the authors of [DenseNet](https://arxiv.org/abs/1608.06993)). This not only allows for increased depth of the network, but also enforces more complex structure. This way the network can learn finer details. Additionally, ESRGAN does not use batch normalization. Learning how normalize the data distribution between layers is a general practice in many Deep Neural Networks. However, in case of SR algorithms (especially the ones which use GANs), it tends to introduce unpleasant artifacts and limits the generalization ability. Removing batch normalization improves the stability and reduces computational cost (less parameters to learn).


__Replacing an ordinary discriminator with the relativistic disciminator__ - it is really interesting that the idea of relativistic discriminator has been already employed by the community shortly after the paper has been published. Using the Relativistic average Discriminator allows the network not only to receive gradients from generated data, but also from the real data. This improves the quality of edges and textures.

{:refdef: style="text-align: center;"}
![alt text](/assets/5/10.png)
{: refdef}
<em>As we go deeper, the layers after activation tend to give us much less information. This results in weak supervision and inferior performance. Therefore, it is more beneficial to use pre-activation feature maps.</em> 

__Revisit perceptual loss__ - the perceptual loss attempts to compare perceptual similarity between the reconstructed image $$G(x_LR)$$  and the ground truth image $$x_HR$$. By running both inputs through the pre-trained VGG network, we receive their representation in form of feature maps after j-th convolution and activation $$\theta(G(x_LR))$$ and $$\theta(x_HR)$$. One of the tasks of the SRGAN was to minimize the difference between those representations. This is still the case in ESRGAN. However, we take the representation after j-th convolution but __before activation__. 

{:refdef: style="text-align: center;"}
![alt text](/assets/5/11.png)
{: refdef}
<em>Interestingly, post-activation feature maps also cause inconsistent reconstructed brightness compared with the GT image.</em> 

__Network interpolation__ - as I have mentioned before, there are two goals which the algorithm tries to achieve. This is not only perceptual similarity between generated image and ground truth, but also lowest possible PSNR. This why initially the network is being trained to minimize PSNR (using L1 loss). Then, the pre-trained network is being used to initialize the generator. This not only allows to avoid undesired local minima for the generator, but also provides the discriminator with quite good super-resolved images from the start. 
The authors state that the best results can be obtained through interpolation between the weights of the initial network (after PSNR optimization) and final network (after GAN training). This allows to control the PSNR versus perceptual similarity trade-off.

### Results:

The experiments are similar to the ones conducted on SRGAN. The goal is to scale the LR image by the factor of 4 and obtain a good quality SR image of size 128x128.

{:refdef: style="text-align: center;"}
![alt text](/assets/5/12.png)
{: refdef}
<em>At the moment, ESRGAN is the state of the art technique for super-resolution.</em> 

{:refdef: style="text-align: center;"}
![alt text](/assets/5/13.png)
{: refdef}
<em>Interpolating between two contradictory goals: minimizing PSNR or maximizing perceptual similarity</em> 

The authors have tested their network at the PIRM-SR challenge, where the ESRGAN has won the first place with the best perceptual index.

<em>All the figures are taken from the publications, which I refer to in my blog post<em>
www.dw.com
 




